## Gradient descent

<center>胡家田<center\>

### 梯度下降法概述

梯度下降法（Gradient Descent）是一种用于最优化问题的迭代优化算法。它在机器学习和深度学习中经常用于调整模型参数以最小化损失函数（成本函数）。梯度下降法的目标是找到损失函数的最小值，从而使模型的预测与实际观察值尽可能接近。

梯度下降法的基本思想是通过计算损失函数对模型参数的梯度（或导数），然后沿着梯度的方向调整参数，以减小损失函数的值。这个过程迭代进行，直到达到损失函数的局部最小值或收敛到某个预定的停止条件。

具体来说，梯度下降法包括以下关键步骤：

1. **初始化参数**：首先，随机或根据某种规则初始化模型参数。
2. **计算损失**：使用当前参数值，计算损失函数的值，这表示模型的性能有多差。
3. **计算梯度**：计算损失函数对每个参数的偏导数（梯度）。梯度告诉我们，如果我们略微改变参数值，损失函数会如何改变。
4. **参数更新**：根据梯度的信息，更新参数，以减小损失函数的值。更新的规则通常是：新参数 = 旧参数 - 学习率 x 梯度。
5. **重复迭代**：重复步骤 2 到 4，直到损失函数收敛或达到停止条件。停止条件可以是达到最大迭代次数、损失函数的变化非常小，或其他条件。

梯度下降法有不同的变体，包括批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent）和小批量梯度下降（Mini-Batch Gradient Descent）。它们在计算梯度时使用不同的数据子集，以满足不同的需求。

梯度下降法是一种非常有效的优化算法，广泛用于训练机器学习模型，特别是深度神经网络。通过不断调整参数，模型可以逐渐逼近最优解，从而提高模型的性能。然而，梯度下降法的性能和稳定性与学习率的选择、初始参数值和数据集的特性等因素密切相关，因此需要谨慎调整算法参数以获得最佳结果。